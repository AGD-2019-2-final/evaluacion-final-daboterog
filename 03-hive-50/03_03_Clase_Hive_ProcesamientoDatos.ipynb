{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext bigdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeout 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Los datos se encuentran almacenados en la carpeta drivers del directorio actual. A continución se procede a crear la carpeta /tmp/drivers en el sistema de archivos de Hadoop (HDFS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-13 22:55:32--  https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/drivers/drivers.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.48.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.48.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2043 (2.0K) [text/plain]\n",
      "Saving to: 'drivers.csv.3'\n",
      "\n",
      "drivers.csv.3       100%[===================>]   2.00K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2019-12-13 22:55:33 (2.72 MB/s) - 'drivers.csv.3' saved [2043/2043]\n",
      "\n",
      "--2019-12-13 22:55:33--  https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/drivers/timesheet.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.48.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.48.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 26205 (26K) [text/plain]\n",
      "Saving to: 'timesheet.csv.3'\n",
      "\n",
      "timesheet.csv.3     100%[===================>]  25.59K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2019-12-13 22:55:33 (846 KB/s) - 'timesheet.csv.3' saved [26205/26205]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/drivers/drivers.csv\n",
    "!wget https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/drivers/timesheet.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 root supergroup       2043 2019-12-13 22:55 /tmp/drivers/drivers.csv\n",
      "-rw-r--r--   1 root supergroup      26205 2019-12-13 22:55 /tmp/drivers/timesheet.csv\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Crea la carpeta drivers en el HDFS\n",
    "##\n",
    "!hdfs dfs -mkdir /tmp/drivers\n",
    "\n",
    "##\n",
    "## Copia los archivos al HDFS\n",
    "##\n",
    "!hdfs dfs -copyFromLocal drivers.csv  /tmp/drivers/\n",
    "!hdfs dfs -copyFromLocal timesheet.csv  /tmp/drivers/\n",
    "\n",
    "##\n",
    "## Lista los archivos al HDFS para verificar\n",
    "## que los archivos fueron copiados correctamente.\n",
    "##\n",
    "!hdfs dfs -ls /tmp/drivers/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## El contenido de un archivo puede ser visualizado parcialmente usando el comando tail. Se usa para realizar una inspección rápida del contenido de los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box 213- 8948 Nec Ave,Y,hours\n",
      "27,Mark Lochbihler,392603159,8355 Ipsum St.,Y,hours\n",
      "28,Olivier Renault,959908181,P.O. Box 243- 6509 Erat. Avenue,Y,hours\n",
      "29,Teddy Choi,185502192,P.O. Box 106- 7003 Amet Rd.,Y,hours\n",
      "30,Dan Rice,282307061,Ap #881-9267 Mollis Avenue,Y,hours\n",
      "31,Rommel Garcia,858912101,P.O. Box 945- 6015 Sociis St.,Y,hours\n",
      "32,Ryan Templeton,290304287,765-6599 Egestas. Av.,Y,hours\n",
      "33,Sridhara Sabbella,967409015,Ap #477-2507 Sagittis Avenue,Y,hours\n",
      "34,Frank Romano,391407216,Ap #753-6814 Quis Ave,Y,hours\n",
      "35,Emil Siemes,971401151,321-2976 Felis Rd.,Y,hours\n",
      "36,Andrew Grande,245303216,Ap #685-9598 Egestas Rd.,Y,hours\n",
      "37,Wes Floyd,190504074,P.O. Box 269- 9611 Nulla Street,Y,hours\n",
      "38,Scott Shaw,386411175,276 Lobortis Road,Y,hours\n",
      "39,David Kaiser,967706052,9185 At Street,Y,hours\n",
      "40,Nicolas Maillard,208510217,1027 Quis Rd.,Y,hours\n",
      "41,Greg Phillips,308103116,P.O. Box 847- 5961 Arcu. Road,Y,hours\n",
      "42,Randy Gelhausen,853302254,145-4200 In- Avenue,Y,hours\n",
      "43,Dave Patton,977706052,3028 A- St.,Y,hours"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Se imprime el final del archivo drivers\n",
    "##\n",
    "!hdfs dfs -tail /tmp/drivers/drivers.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42,36,56,2612\n",
      "42,37,48,2550\n",
      "42,38,55,2527\n",
      "42,39,57,2723\n",
      "42,40,55,2728\n",
      "42,41,50,2557\n",
      "42,42,53,2773\n",
      "42,43,55,2786\n",
      "42,44,54,2638\n",
      "42,45,57,2542\n",
      "42,46,48,2526\n",
      "42,47,50,2795\n",
      "42,48,53,2609\n",
      "42,49,58,2584\n",
      "42,50,48,2692\n",
      "42,51,50,2566\n",
      "42,52,48,2735\n",
      "43,1,46,2622\n",
      "43,2,47,2688\n",
      "43,3,50,2544\n",
      "43,4,56,2573\n",
      "43,5,54,2691\n",
      "43,6,52,2796\n",
      "43,7,53,2564\n",
      "43,8,58,2624\n",
      "43,9,50,2528\n",
      "43,10,57,2721\n",
      "43,11,51,2722\n",
      "43,12,59,2681\n",
      "43,13,52,2683\n",
      "43,14,46,2663\n",
      "43,15,53,2579\n",
      "43,16,56,2519\n",
      "43,17,54,2584\n",
      "43,18,47,2665\n",
      "43,19,55,2511\n",
      "43,20,60,2677\n",
      "43,21,52,2585\n",
      "43,22,60,2719\n",
      "43,23,48,2655\n",
      "43,24,48,2641\n",
      "43,25,53,2512\n",
      "43,26,48,2612\n",
      "43,27,58,2614\n",
      "43,28,60,2551\n",
      "43,29,55,2682\n",
      "43,30,49,2504\n",
      "43,31,51,2701\n",
      "43,32,57,2554\n",
      "43,33,52,2730\n",
      "43,34,54,2783\n",
      "43,35,51,2681\n",
      "43,36,51,2655\n",
      "43,37,46,2629\n",
      "43,38,58,2739\n",
      "43,39,47,2535\n",
      "43,40,50,2512\n",
      "43,41,51,2701\n",
      "43,42,55,2538\n",
      "43,43,58,2775\n",
      "43,44,56,2545\n",
      "43,45,46,2671\n",
      "43,46,57,2680\n",
      "43,47,50,2572\n",
      "43,48,52,2517\n",
      "43,49,56,2743\n",
      "43,50,59,2665\n",
      "43,51,58,2593\n",
      "43,52,48,2764"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -tail /tmp/drivers/timesheet.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creación de la tabla temp_drivers\n",
    "##   A continuación se crea la tabla temp_drivers, que es almacenada en el disco como un archivo de texto, para almacenar la información de los conductores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS temp_drivers;\n",
      "OK\n",
      "Time taken: 7.962 seconds\n",
      "CREATE TABLE temp_drivers (col_value STRING) STORED AS TEXTFILE\n",
      "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
      "OK\n",
      "Time taken: 0.811 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "DROP TABLE IF EXISTS temp_drivers;\n",
    "CREATE TABLE temp_drivers (col_value STRING) STORED AS TEXTFILE\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seguidamente, se visualizan las tablas en la base de datos actual que empiezan por t para verificar que la tabla fue creada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOW TABLES LIKE 't*';\n",
      "OK\n",
      "temp_drivers\n",
      "Time taken: 0.135 seconds, Fetched: 1 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SHOW TABLES LIKE 't*';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Carga de datos para la tabla temp_drivers\n",
    "##   La siguiente consulta realiza la carga de los datos del archivo drivers.csv en la tabla temp_drivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ers; DATA INPATH '/tmp/drivers/drivers.csv' OVERWRITE INTO TABLE temp_driv \n",
      "Loading data to table default.temp_drivers\n",
      "OK\n",
      "Time taken: 0.881 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "LOAD DATA INPATH '/tmp/drivers/drivers.csv' OVERWRITE INTO TABLE temp_drivers;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hive consume los datos, es decir, mueve los datos a la bodega de datos, de tal forma que el archivo drivers.csv es eliminado de la carpeta /tmp/drivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 root supergroup      26205 2019-12-13 22:55 /tmp/drivers/timesheet.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /tmp/drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se obtiene los primeros 10 registros de la tabla para realizar una inspección rápida de los datos y verificar que los datos fueron cargados correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM temp_drivers LIMIT 10;\n",
      "OK\n",
      "10,George Vetticaden,621011971,244-4532 Nulla Rd.,N,miles\n",
      "11,Jamie Engesser,262112338,366-4125 Ac Street,N,miles\n",
      "12,Paul Coddin,198041975,Ap #622-957 Risus. Street,Y,hours\n",
      "13,Joe Niemiec,139907145,2071 Hendrerit. Ave,Y,hours\n",
      "14,Adis Cesir,820812209,Ap #810-1228 In St.,Y,hours\n",
      "15,Rohit Bakshi,239005227,648-5681 Dui- Rd.,Y,hours\n",
      "16,Tom McCuch,363303105,P.O. Box 313- 962 Parturient Rd.,Y,hours\n",
      "17,Eric Mizell,123808238,P.O. Box 579- 2191 Gravida. Street,Y,hours\n",
      "18,Grant Liu,171010151,Ap #928-3159 Vestibulum Av.,Y,hours\n",
      "19,Ajay Singh,160005158,592-9430 Nonummy Avenue,Y,hours\n",
      "Time taken: 1.909 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT * FROM temp_drivers LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creación de la tabla drivers\n",
    "##   A continuación se crea la tabla drivers en donde se colocará la información extraída de la tabla temp_drivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS drivers;\n",
      "OK\n",
      "Time taken: 0.006 seconds\n",
      "CREATE TABLE drivers (driverId  INT,\n",
      "                      name      STRING,\n",
      "                      ssn       BIGINT,\n",
      "                      location  STRING,\n",
      "                      certified STRING,\n",
      "                      wageplan  STRING)\n",
      "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
      "OK\n",
      "Time taken: 0.05 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "\n",
    "DROP TABLE IF EXISTS drivers;\n",
    "\n",
    "CREATE TABLE drivers (driverId  INT,\n",
    "                      name      STRING,\n",
    "                      ssn       BIGINT,\n",
    "                      location  STRING,\n",
    "                      certified STRING,\n",
    "                      wageplan  STRING)\n",
    "\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ya que cada registro de la tabla temp_drivers es una línea de texto, se aplica una expresión regular (regexp_extract) para realizar la división del texto por las comas.\n",
    "## La parte {1} representa la primera cadena de caracteres después de realizar la partición, \n",
    "## {2} la segunda y así sucesivamente. \n",
    "## Después de la llamada a la función regexp_extract se indica el nombre de la columna en la tabla drivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT OVERWRITE TABLE drivers\n",
      "SELECT\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){1}', 1) driverId,\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){2}', 1) name,\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){3}', 1) ssn,\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){4}', 1) location,\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){5}', 1) certified,\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){6}', 1) wageplan\n",
      "FROM\n",
      "    temp_drivers;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20191213225729_ae9b4c00-8836-4846-a4a8-57658fe7cc80\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1576277597711_0001, Tracking URL = http://6ef480276af8:8088/proxy/application_1576277597711_0001/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1576277597711_0001\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2019-12-13 22:57:41,499 Stage-1 map = 0%,  reduce = 0%\n",
      "2019-12-13 22:57:46,930 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.3 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 300 msec\n",
      "Ended Job = job_1576277597711_0001\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to directory hdfs://0.0.0.0:9000/user/hive/warehouse/drivers/.hive-staging_hive_2019-12-13_22-57-29_577_8500957941223920632-1/-ext-10000\n",
      "Loading data to table default.drivers\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 2.3 sec   HDFS Read: 6812 HDFS Write: 2036 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 2 seconds 300 msec\n",
      "OK\n",
      "Time taken: 19.893 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "INSERT OVERWRITE TABLE drivers\n",
    "SELECT\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){1}', 1) driverId,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){2}', 1) name,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){3}', 1) ssn,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){4}', 1) location,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){5}', 1) certified,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){6}', 1) wageplan\n",
    "FROM\n",
    "    temp_drivers;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se aplica la instrucción SELECT para revisar el resultado de la carga de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM drivers LIMIT 10;\n",
      "OK\n",
      "11\tJamie Engesser\t262112338\t366-4125 Ac Street\tN\tmiles\n",
      "12\tPaul Coddin\t198041975\tAp #622-957 Risus. Street\tY\thours\n",
      "13\tJoe Niemiec\t139907145\t2071 Hendrerit. Ave\tY\thours\n",
      "14\tAdis Cesir\t820812209\tAp #810-1228 In St.\tY\thours\n",
      "15\tRohit Bakshi\t239005227\t648-5681 Dui- Rd.\tY\thours\n",
      "16\tTom McCuch\t363303105\tP.O. Box 313- 962 Parturient Rd.\tY\thours\n",
      "17\tEric Mizell\t123808238\tP.O. Box 579- 2191 Gravida. Street\tY\thours\n",
      "18\tGrant Liu\t171010151\tAp #928-3159 Vestibulum Av.\tY\thours\n",
      "19\tAjay Singh\t160005158\t592-9430 Nonummy Avenue\tY\thours\n",
      "20\tChris Harris\t921812303\t883-2691 Proin Avenue\tY\thours\n",
      "Time taken: 0.119 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT * FROM drivers LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creación de la tabla temp_timesheet\n",
    "##   Se procede a crear la tabla y cargar los datos para el archivo time_sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS temp_timesheet;\n",
      "OK\n",
      "Time taken: 0.007 seconds\n",
      "CREATE TABLE temp_timesheet (col_value string)\n",
      "STORED AS TEXTFILE\n",
      "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
      "OK\n",
      "Time taken: 0.039 seconds\n",
      "mesheet;A INPATH '/tmp/drivers/timesheet.csv' OVERWRITE INTO TABLE temp_ti \n",
      "Loading data to table default.temp_timesheet\n",
      "OK\n",
      "Time taken: 0.369 seconds\n",
      "SELECT * FROM temp_timesheet LIMIT 10;\n",
      "OK\n",
      "10,1,70,3300\n",
      "10,2,70,3300\n",
      "10,3,60,2800\n",
      "10,4,70,3100\n",
      "10,5,70,3200\n",
      "10,6,70,3300\n",
      "10,7,70,3000\n",
      "10,8,70,3300\n",
      "10,9,70,3200\n",
      "10,10,50,2500\n",
      "Time taken: 0.13 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "\n",
    "DROP TABLE IF EXISTS temp_timesheet;\n",
    "\n",
    "CREATE TABLE temp_timesheet (col_value string)\n",
    "STORED AS TEXTFILE\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
    "\n",
    "LOAD DATA INPATH '/tmp/drivers/timesheet.csv' OVERWRITE INTO TABLE temp_timesheet;\n",
    "\n",
    "SELECT * FROM temp_timesheet LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creación de la tabla timesheet\n",
    "##   Se procede igual que en las tablas anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS timesheet;\n",
      "OK\n",
      "Time taken: 0.005 seconds\n",
      "ogged INT)LE timesheet (driverId INT, week INT, hours_logged INT , miles_l \n",
      "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
      "OK\n",
      "Time taken: 0.035 seconds\n",
      "INSERT OVERWRITE TABLE timesheet\n",
      "SELECT\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){1}', 1) driverId,\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){2}', 1) week,\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){3}', 1) hours_logged,\n",
      "    regexp_extract(col_value, '^(?:([^,]*),?){4}', 1) miles_logged\n",
      "FROM\n",
      "    temp_timesheet;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20191213225808_d0987c88-a8ca-4780-9f47-cdd4859f9726\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1576277597711_0002, Tracking URL = http://6ef480276af8:8088/proxy/application_1576277597711_0002/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1576277597711_0002\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2019-12-13 22:58:15,013 Stage-1 map = 0%,  reduce = 0%\n",
      "2019-12-13 22:58:20,273 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.39 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 390 msec\n",
      "Ended Job = job_1576277597711_0002\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to directory hdfs://0.0.0.0:9000/user/hive/warehouse/timesheet/.hive-staging_hive_2019-12-13_22-58-08_228_7415001761888332665-1/-ext-10000\n",
      "Loading data to table default.timesheet\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 2.39 sec   HDFS Read: 30754 HDFS Write: 24476 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 2 seconds 390 msec\n",
      "OK\n",
      "Time taken: 13.501 seconds\n",
      "SELECT * FROM timesheet LIMIT 10;\n",
      "OK\n",
      "10\t2\t70\t3300\n",
      "10\t3\t60\t2800\n",
      "10\t4\t70\t3100\n",
      "10\t5\t70\t3200\n",
      "10\t6\t70\t3300\n",
      "10\t7\t70\t3000\n",
      "10\t8\t70\t3300\n",
      "10\t9\t70\t3200\n",
      "10\t10\t50\t2500\n",
      "10\t11\t70\t2900\n",
      "Time taken: 0.089 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "\n",
    "DROP TABLE IF EXISTS timesheet;\n",
    "\n",
    "CREATE TABLE timesheet (driverId INT, week INT, hours_logged INT , miles_logged INT)\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
    "\n",
    "INSERT OVERWRITE TABLE timesheet\n",
    "SELECT\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){1}', 1) driverId,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){2}', 1) week,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){3}', 1) hours_logged,\n",
    "    regexp_extract(col_value, '^(?:([^,]*),?){4}', 1) miles_logged\n",
    "FROM\n",
    "    temp_timesheet;\n",
    "\n",
    "SELECT * FROM timesheet LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cantidad de horas y millas de cada conductor por año.\n",
    "##   En la siguiente consulta se desea obtener para cada conductor la cantidad de horas y millas por año."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT\n",
      "    driverId,\n",
      "    sum(hours_logged),\n",
      "    sum(miles_logged)\n",
      "FROM\n",
      "    timesheet\n",
      "GROUP BY\n",
      "    driverId;\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20191213225822_2e8c4a76-3b9c-4999-ba4b-5670f0372254\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1576277597711_0003, Tracking URL = http://6ef480276af8:8088/proxy/application_1576277597711_0003/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1576277597711_0003\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2019-12-13 22:58:32,170 Stage-1 map = 0%,  reduce = 0%\n",
      "2019-12-13 22:58:37,506 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.9 sec\n",
      "2019-12-13 22:58:43,845 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.42 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 420 msec\n",
      "Ended Job = job_1576277597711_0003\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 3.42 sec   HDFS Read: 33422 HDFS Write: 1005 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 3 seconds 420 msec\n",
      "OK\n",
      "10\t3162\t143850\n",
      "11\t3642\t179300\n",
      "12\t2639\t135962\n",
      "13\t2727\t134126\n",
      "14\t2781\t136624\n",
      "15\t2734\t138750\n",
      "16\t2746\t137205\n",
      "17\t2701\t135992\n",
      "18\t2654\t137834\n",
      "19\t2738\t137968\n",
      "20\t2644\t134564\n",
      "21\t2751\t138719\n",
      "22\t2733\t137550\n",
      "23\t2750\t137980\n",
      "24\t2647\t134461\n",
      "25\t2723\t139180\n",
      "26\t2730\t137530\n",
      "27\t2771\t137922\n",
      "28\t2723\t137469\n",
      "29\t2760\t138255\n",
      "30\t2773\t137473\n",
      "31\t2704\t137057\n",
      "32\t2736\t137422\n",
      "33\t2759\t139285\n",
      "34\t2811\t137728\n",
      "35\t2728\t138727\n",
      "36\t2795\t138025\n",
      "37\t2694\t137223\n",
      "38\t2760\t137464\n",
      "39\t2745\t138788\n",
      "40\t2700\t136931\n",
      "41\t2723\t138407\n",
      "42\t2697\t136673\n",
      "43\t2750\t136993\n",
      "Time taken: 22.592 seconds, Fetched: 34 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT\n",
    "    driverId,\n",
    "    sum(hours_logged),\n",
    "    sum(miles_logged)\n",
    "FROM\n",
    "    timesheet\n",
    "GROUP BY\n",
    "    driverId;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Consulta para unir las tablas\n",
    "##   El paso final consiste en crear una consulta que agregue el nombre del conductor de la tabla drivers con la cantidad de horas y millas por año."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT\n",
      "    d.driverId,\n",
      "    d.name,\n",
      "    t.total_hours,\n",
      "    t.total_miles\n",
      "FROM\n",
      "    drivers d\n",
      "JOIN (\n",
      "    SELECT\n",
      "        driverId,\n",
      "        sum(hours_logged)total_hours,\n",
      "        sum(miles_logged)total_miles\n",
      "    FROM\n",
      "        timesheet\n",
      "    GROUP BY\n",
      "        driverId\n",
      "    ) t\n",
      "ON\n",
      "    (d.driverId = t.driverId);\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20191213225900_c8fd76b8-32d1-4b9f-ba41-b78cacba5f80\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1576277597711_0004, Tracking URL = http://6ef480276af8:8088/proxy/application_1576277597711_0004/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1576277597711_0004\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2019-12-13 22:59:08,570 Stage-2 map = 0%,  reduce = 0%\n",
      "2019-12-13 22:59:13,873 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.02 sec\n",
      "2019-12-13 22:59:19,152 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.29 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 290 msec\n",
      "Ended Job = job_1576277597711_0004\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "2019-12-13 22:59:26\tStarting to launch local task to process map join;\tmaximum memory = 477626368\n",
      "2019-12-13 22:59:27\tDump the side-table for tag: 0 with group count: 33 into file: file:/tmp/root/a0a0192d-bb80-4bc2-a0ad-95742f46c2a2/hive_2019-12-13_22-59-00_809_2803877731357256714-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable\n",
      "2019-12-13 22:59:27\tUploaded 1 File to: file:/tmp/root/a0a0192d-bb80-4bc2-a0ad-95742f46c2a2/hive_2019-12-13_22-59-00_809_2803877731357256714-1/-local-10005/HashTable-Stage-3/MapJoin-mapfile00--.hashtable (1325 bytes)\n",
      "2019-12-13 22:59:27\tEnd of local task; Time Taken: 1.751 sec.\n",
      "Execution completed successfully\n",
      "MapredLocal task succeeded\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1576277597711_0005, Tracking URL = http://6ef480276af8:8088/proxy/application_1576277597711_0005/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1576277597711_0005\n",
      "Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0\n",
      "2019-12-13 22:59:36,293 Stage-3 map = 0%,  reduce = 0%\n",
      "2019-12-13 22:59:41,842 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 1.98 sec\n",
      "MapReduce Total cumulative CPU time: 1 seconds 980 msec\n",
      "Ended Job = job_1576277597711_0005\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 3.29 sec   HDFS Read: 32521 HDFS Write: 946 SUCCESS\n",
      "Stage-Stage-3: Map: 1   Cumulative CPU: 1.98 sec   HDFS Read: 6784 HDFS Write: 1411 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 5 seconds 270 msec\n",
      "OK\n",
      "11\tJamie Engesser\t3642\t179300\n",
      "12\tPaul Coddin\t2639\t135962\n",
      "13\tJoe Niemiec\t2727\t134126\n",
      "14\tAdis Cesir\t2781\t136624\n",
      "15\tRohit Bakshi\t2734\t138750\n",
      "16\tTom McCuch\t2746\t137205\n",
      "17\tEric Mizell\t2701\t135992\n",
      "18\tGrant Liu\t2654\t137834\n",
      "19\tAjay Singh\t2738\t137968\n",
      "20\tChris Harris\t2644\t134564\n",
      "21\tJeff Markham\t2751\t138719\n",
      "22\tNadeem Asghar\t2733\t137550\n",
      "23\tAdam Diaz\t2750\t137980\n",
      "24\tDon Hilborn\t2647\t134461\n",
      "25\tJean-Philippe Playe\t2723\t139180\n",
      "26\tMichael Aube\t2730\t137530\n",
      "27\tMark Lochbihler\t2771\t137922\n",
      "28\tOlivier Renault\t2723\t137469\n",
      "29\tTeddy Choi\t2760\t138255\n",
      "30\tDan Rice\t2773\t137473\n",
      "31\tRommel Garcia\t2704\t137057\n",
      "32\tRyan Templeton\t2736\t137422\n",
      "33\tSridhara Sabbella\t2759\t139285\n",
      "34\tFrank Romano\t2811\t137728\n",
      "35\tEmil Siemes\t2728\t138727\n",
      "36\tAndrew Grande\t2795\t138025\n",
      "37\tWes Floyd\t2694\t137223\n",
      "38\tScott Shaw\t2760\t137464\n",
      "39\tDavid Kaiser\t2745\t138788\n",
      "40\tNicolas Maillard\t2700\t136931\n",
      "41\tGreg Phillips\t2723\t138407\n",
      "42\tRandy Gelhausen\t2697\t136673\n",
      "43\tDave Patton\t2750\t136993\n",
      "Time taken: 42.124 seconds, Fetched: 33 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "SELECT\n",
    "    d.driverId,\n",
    "    d.name,\n",
    "    t.total_hours,\n",
    "    t.total_miles\n",
    "FROM\n",
    "    drivers d\n",
    "JOIN (\n",
    "    SELECT\n",
    "        driverId,\n",
    "        sum(hours_logged)total_hours,\n",
    "        sum(miles_logged)total_miles\n",
    "    FROM\n",
    "        timesheet\n",
    "    GROUP BY\n",
    "        driverId\n",
    "    ) t\n",
    "ON\n",
    "    (d.driverId = t.driverId);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Almacenamiento de los resultados\n",
    "##   Finalmente, se agrega una porción de codigo adicional a la consulta anterior para almacenar la tabla final obtenida en la carpeta /tmp/drivers/summary del HDFS \n",
    "##   para que otras aplicaciones puedan usar estos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT OVERWRITE DIRECTORY '/tmp/drivers/summary'\n",
      "ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
      "SELECT\n",
      "    d.driverId,\n",
      "    d.name,\n",
      "    t.total_hours,\n",
      "    t.total_miles\n",
      "FROM\n",
      "    drivers d\n",
      "JOIN (\n",
      "    SELECT\n",
      "        driverId,\n",
      "        sum(hours_logged)total_hours,\n",
      "        sum(miles_logged)total_miles\n",
      "    FROM\n",
      "        timesheet\n",
      "    GROUP BY\n",
      "        driverId\n",
      "    ) t\n",
      "ON\n",
      "    (d.driverId = t.driverId);\n",
      "WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.\n",
      "Query ID = root_20191213230031_5b8c7d99-0606-4a92-9ef3-ccbb6569b425\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1576277597711_0006, Tracking URL = http://6ef480276af8:8088/proxy/application_1576277597711_0006/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1576277597711_0006\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2019-12-13 23:00:38,212 Stage-2 map = 0%,  reduce = 0%\n",
      "2019-12-13 23:00:44,535 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 2.12 sec\n",
      "2019-12-13 23:00:49,855 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.74 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 740 msec\n",
      "Ended Job = job_1576277597711_0006\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "2019-12-13 23:00:57\tStarting to launch local task to process map join;\tmaximum memory = 477626368\n",
      "2019-12-13 23:00:59\tDump the side-table for tag: 0 with group count: 33 into file: file:/tmp/root/a0a0192d-bb80-4bc2-a0ad-95742f46c2a2/hive_2019-12-13_23-00-31_621_1798972370574930889-1/-local-10003/HashTable-Stage-3/MapJoin-mapfile10--.hashtable\n",
      "2019-12-13 23:00:59\tUploaded 1 File to: file:/tmp/root/a0a0192d-bb80-4bc2-a0ad-95742f46c2a2/hive_2019-12-13_23-00-31_621_1798972370574930889-1/-local-10003/HashTable-Stage-3/MapJoin-mapfile10--.hashtable (1325 bytes)\n",
      "2019-12-13 23:00:59\tEnd of local task; Time Taken: 1.831 sec.\n",
      "Execution completed successfully\n",
      "MapredLocal task succeeded\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1576277597711_0007, Tracking URL = http://6ef480276af8:8088/proxy/application_1576277597711_0007/\n",
      "Kill Command = /usr/local/hadoop/bin/hadoop job  -kill job_1576277597711_0007\n",
      "Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0\n",
      "2019-12-13 23:01:08,309 Stage-3 map = 0%,  reduce = 0%\n",
      "2019-12-13 23:01:12,918 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 1.89 sec\n",
      "MapReduce Total cumulative CPU time: 1 seconds 890 msec\n",
      "Ended Job = job_1576277597711_0007\n",
      "Moving data to directory /tmp/drivers/summary\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 3.74 sec   HDFS Read: 32538 HDFS Write: 946 SUCCESS\n",
      "Stage-Stage-3: Map: 1   Cumulative CPU: 1.89 sec   HDFS Read: 6345 HDFS Write: 928 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 5 seconds 630 msec\n",
      "OK\n",
      "Time taken: 43.931 seconds\n"
     ]
    }
   ],
   "source": [
    "%%hive\n",
    "INSERT OVERWRITE DIRECTORY '/tmp/drivers/summary'\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "SELECT\n",
    "    d.driverId,\n",
    "    d.name,\n",
    "    t.total_hours,\n",
    "    t.total_miles\n",
    "FROM\n",
    "    drivers d\n",
    "JOIN (\n",
    "    SELECT\n",
    "        driverId,\n",
    "        sum(hours_logged)total_hours,\n",
    "        sum(miles_logged)total_miles\n",
    "    FROM\n",
    "        timesheet\n",
    "    GROUP BY\n",
    "        driverId\n",
    "    ) t\n",
    "ON\n",
    "    (d.driverId = t.driverId);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rwxr-xr-x   1 root supergroup        928 2019-11-27 21:36 /tmp/drivers/summary/000000_0\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /tmp/drivers/summary/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11,Jamie Engesser,3642,179300\n",
      "12,Paul Coddin,2639,135962\n",
      "13,Joe Niemiec,2727,134126\n",
      "14,Adis Cesir,2781,136624\n",
      "15,Rohit Bakshi,2734,138750\n",
      "16,Tom McCuch,2746,137205\n",
      "17,Eric Mizell,2701,135992\n",
      "18,Grant Liu,2654,137834\n",
      "19,Ajay Singh,2738,137968\n",
      "20,Chris Harris,2644,134564\n",
      "21,Jeff Markham,2751,138719\n",
      "22,Nadeem Asghar,2733,137550\n",
      "23,Adam Diaz,2750,137980\n",
      "24,Don Hilborn,2647,134461\n",
      "25,Jean-Philippe Playe,2723,139180\n",
      "26,Michael Aube,2730,137530\n",
      "27,Mark Lochbihler,2771,137922\n",
      "28,Olivier Renault,2723,137469\n",
      "29,Teddy Choi,2760,138255\n",
      "30,Dan Rice,2773,137473\n",
      "31,Rommel Garcia,2704,137057\n",
      "32,Ryan Templeton,2736,137422\n",
      "33,Sridhara Sabbella,2759,139285\n",
      "34,Frank Romano,2811,137728\n",
      "35,Emil Siemes,2728,138727\n",
      "36,Andrew Grande,2795,138025\n",
      "37,Wes Floyd,2694,137223\n",
      "38,Scott Shaw,2760,137464\n",
      "39,David Kaiser,2745,138788\n",
      "40,Nicolas Maillard,2700,136931\n",
      "41,Greg Phillips,2723,138407\n",
      "42,Randy Gelhausen,2697,136673\n",
      "43,Dave Patton,2750,136993\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -tail /tmp/drivers/summary/000000_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'derby.log': Text file busy\n",
      "rm: cannot remove 'ubuntu-bionic-18.04-cloudimg-console.log': Text file busy\n"
     ]
    }
   ],
   "source": [
    "!rm *.csv *.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finalmente, y una vez se ha terminado de depurar el código, se cierra el interprete de Hive que se abrió en el background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%hive_quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
