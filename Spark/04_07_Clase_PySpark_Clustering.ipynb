{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Segmentación del mercado de adolecentes en PySpark\n",
    "##   En este tutorial se aplica el algoritmo K-means para clasificar un grupo de adolecentes con base en sus intéreses, con el fin de diseñar estrategias publicitarias \n",
    "##   y servicios encaminados a cada grupo de interés usando PySpark. Este tutorial se enfoca en la programación de PySpark y no en el análisis del problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definición del problema\n",
    "##   Un vendedor desea enviar publicidad electrónica a una población de adolecentes y adultos jóvenes con el fin de maximizar sus ventas. Para ello, desea poder clasificar \n",
    "##   a sus clientes potenciales por grupos de interés de acuerdo con sus intereses y consecuentemente enviar publicidad específica a cada uno de ellos.\n",
    "##   En este problema se desea determina que grupos de interés existen en una población de clientes a partir de los mensajes enviados por un servicio de redes sociales. \n",
    "##   La información disponible consiste en 30000 observaciones de 40 variables que podrían caracterizar los intereses de la población analizada. Estas variables corresponden \n",
    "##   a palabras que pueden asociarse a un interés de la poblaión analizada. Cada variable mide la frecuencia con que una determinada palabra aparece en los mensajes de texto; \n",
    "##   adicionalmente, dentro de estas variables se incluye información como el sexo, la edad y la cantidad de contactos de la persona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Se cargan librerías auxiliares que\n",
    "## pueden ser de utilidad en el desarrollo\n",
    "## del código\n",
    "##\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Carga de las librerías de Spark\n",
    "##\n",
    "import findspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "APP_NAME = \"spark-kmeans-app\"\n",
    "\n",
    "conf = SparkConf().setAppName(APP_NAME)\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## El archivo con los datos se encuentra en la carpeta actual de trabajo en la máquina local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-01-25 15:38:03--  https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/snsdata.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.48.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.48.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2631136 (2.5M) [text/plain]\n",
      "Saving to: 'snsdata.csv'\n",
      "\n",
      "snsdata.csv         100%[===================>]   2.51M   145KB/s    in 61s     \n",
      "\n",
      "2020-01-25 15:39:42 (42.2 KB/s) - 'snsdata.csv' saved [2631136/2631136]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/jdvelasq/datalabs/master/datasets/snsdata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradyear,gender,age,friends,basketball,football,soccer,softball,volleyball,swimming,cheerleading,baseball,tennis,sports,cute,sex,sexy,hot,kissed,dance,band,marching,music,rock,god,church,jesus,bible,hair,dress,blonde,mall,shopping,clothes,hollister,abercrombie,die,death,drunk,drugs\n",
      "2006,M,18.982,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "2006,F,18.801,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,2,2,1,0,0,0,6,4,0,1,0,0,0,0,0,0,0,0\n",
      "2006,M,18.335,69,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0\n",
      "2006,F,18.875,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "2006,NA,18.995,10,0,0,0,0,0,0,0,0,0,0,0,1,0,0,5,1,1,0,3,0,1,0,0,0,1,0,0,0,2,0,0,0,0,0,1,1\n",
      "2006,F,,142,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,2,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0\n",
      "2006,F,18.93,72,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,2,0,0,2,0,0,0,0,0\n",
      "2006,M,18.322,17,0,0,0,1,0,0,0,0,0,0,0,2,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
      "2006,F,19.055,52,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Contenido del archivo\n",
    "##\n",
    "!head snsdata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Mueve el archivo de datos al hdfs\n",
    "##\n",
    "!hdfs dfs -copyFromLocal snsdata.csv /tmp/snsdata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## Se carga el archivo en PySpark\n",
    "##\n",
    "spark_df = spark.read.load(\"/tmp/snsdata.csv\",\n",
    "                           format=\"csv\",\n",
    "                           sep=\",\",\n",
    "                           inferSchema=\"true\",\n",
    "                           header=\"true\")\n",
    "\n",
    "##\n",
    "## Número de registros cargados\n",
    "##\n",
    "spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gradyear: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- friends: integer (nullable = true)\n",
      " |-- basketball: integer (nullable = true)\n",
      " |-- football: integer (nullable = true)\n",
      " |-- soccer: integer (nullable = true)\n",
      " |-- softball: integer (nullable = true)\n",
      " |-- volleyball: integer (nullable = true)\n",
      " |-- swimming: integer (nullable = true)\n",
      " |-- cheerleading: integer (nullable = true)\n",
      " |-- baseball: integer (nullable = true)\n",
      " |-- tennis: integer (nullable = true)\n",
      " |-- sports: integer (nullable = true)\n",
      " |-- cute: integer (nullable = true)\n",
      " |-- sex: integer (nullable = true)\n",
      " |-- sexy: integer (nullable = true)\n",
      " |-- hot: integer (nullable = true)\n",
      " |-- kissed: integer (nullable = true)\n",
      " |-- dance: integer (nullable = true)\n",
      " |-- band: integer (nullable = true)\n",
      " |-- marching: integer (nullable = true)\n",
      " |-- music: integer (nullable = true)\n",
      " |-- rock: integer (nullable = true)\n",
      " |-- god: integer (nullable = true)\n",
      " |-- church: integer (nullable = true)\n",
      " |-- jesus: integer (nullable = true)\n",
      " |-- bible: integer (nullable = true)\n",
      " |-- hair: integer (nullable = true)\n",
      " |-- dress: integer (nullable = true)\n",
      " |-- blonde: integer (nullable = true)\n",
      " |-- mall: integer (nullable = true)\n",
      " |-- shopping: integer (nullable = true)\n",
      " |-- clothes: integer (nullable = true)\n",
      " |-- hollister: integer (nullable = true)\n",
      " |-- abercrombie: integer (nullable = true)\n",
      " |-- die: integer (nullable = true)\n",
      " |-- death: integer (nullable = true)\n",
      " |-- drunk: integer (nullable = true)\n",
      " |-- drugs: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Tipos de datos de los campos del DataFrame\n",
    "##\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Análisis exploratorio\n",
    "##   A continuación se ejemplifican algunos cómputos típicos para el análisis exploratorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>22054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NA</td>\n",
       "      <td>2724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>5222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender  count\n",
       "0      F  22054\n",
       "1     NA   2724\n",
       "2      M   5222"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## Conteo por género\n",
    "##\n",
    "spark_df.groupBy('gender').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>24914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>17.993949546439772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>7.858054477853863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>3.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>106.927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                 age\n",
       "0   count               24914\n",
       "1    mean  17.993949546439772\n",
       "2  stddev   7.858054477853863\n",
       "3     min               3.086\n",
       "4     max             106.927"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## Se analizan los rangos de las variables\n",
    "## para determinar si hay datos por fuera de sus\n",
    "## rangos válidos. La variable `edad` contiene\n",
    "## datos por fuera de la población de interés.\n",
    "##\n",
    "## La muestra contiene un rango de edades\n",
    "## por fuera de la población de interés\n",
    "##\n",
    "spark_df.select('age').describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   age|\n",
      "+------+\n",
      "|18.982|\n",
      "|18.801|\n",
      "|18.335|\n",
      "|18.875|\n",
      "|18.995|\n",
      "|  null|\n",
      "| 18.93|\n",
      "|18.322|\n",
      "|19.055|\n",
      "|18.708|\n",
      "|18.543|\n",
      "|19.463|\n",
      "|18.097|\n",
      "|  null|\n",
      "|18.398|\n",
      "|  null|\n",
      "|  null|\n",
      "|18.987|\n",
      "|17.158|\n",
      "|18.497|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Se imprimen algunos registros para ver los\n",
    "## valores del campo edad\n",
    "##\n",
    "spark_df.select('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5086"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## Cantidad de nulos en la columna age\n",
    "##\n",
    "spark_df.filter(spark_df['age'].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>age1319</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>24477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>17.25242893328433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>1.1574649278955391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>13.027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>19.995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary             age1319\n",
       "0   count               24477\n",
       "1    mean   17.25242893328433\n",
       "2  stddev  1.1574649278955391\n",
       "3     min              13.027\n",
       "4     max              19.995"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## Se seleccionan las personas entre 13 y 20 años y\n",
    "## se descartan las demás observaciones\n",
    "##\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "##\n",
    "## Se agrega una columna con las edades entre 13 y 19,\n",
    "## reemplazando por null los valores por fuera de este\n",
    "## rango\n",
    "##\n",
    "spark_df = spark_df.withColumn(\n",
    "    'age1319',\n",
    "    when((spark_df['age'] >= 13) & (spark_df['age'] < 20),\n",
    "         spark_df['age']\n",
    "        ).otherwise(lit(None).cast(DoubleType())))\n",
    "\n",
    "##\n",
    "## Se verifican los valores en la nueva columna\n",
    "##\n",
    "spark_df.select('age1319').describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|gradyear|      avg(age1319)|\n",
      "+--------+------------------+\n",
      "|    2006|18.655857950872626|\n",
      "|    2007| 17.70617237497992|\n",
      "|    2008|16.767700737100785|\n",
      "|    2009|15.819573344509596|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Se calcula la edad promedio por año\n",
    "## de graduación para la muestra en el\n",
    "## rango de edades considerado\n",
    "##\n",
    "age_df = spark_df.groupby(\"gradyear\").mean().select(['gradyear', 'avg(age1319)']).orderBy('gradyear')\n",
    "age_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nombres de las columnas de interés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['friends',\n",
       " 'basketball',\n",
       " 'football',\n",
       " 'soccer',\n",
       " 'softball',\n",
       " 'volleyball',\n",
       " 'swimming',\n",
       " 'cheerleading',\n",
       " 'baseball',\n",
       " 'tennis',\n",
       " 'sports',\n",
       " 'cute',\n",
       " 'sex',\n",
       " 'sexy',\n",
       " 'hot',\n",
       " 'kissed',\n",
       " 'dance',\n",
       " 'band',\n",
       " 'marching',\n",
       " 'music',\n",
       " 'rock',\n",
       " 'god',\n",
       " 'church',\n",
       " 'jesus',\n",
       " 'bible',\n",
       " 'hair',\n",
       " 'dress',\n",
       " 'blonde',\n",
       " 'mall',\n",
       " 'shopping',\n",
       " 'clothes',\n",
       " 'hollister',\n",
       " 'abercrombie',\n",
       " 'die',\n",
       " 'death',\n",
       " 'drunk',\n",
       " 'drugs',\n",
       " 'age1319']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## Se obtienen las columnas de las\n",
    "## características de interes\n",
    "##\n",
    "inputCols = [a for a,_ in spark_df.dtypes]\n",
    "inputCols = inputCols[3:-1]\n",
    "inputCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensamble de las columnas usadas en el clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se ensamblan las columnas en una lista por cada registro del DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Output column rawFeatures already exists.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o204.transform.\n: java.lang.IllegalArgumentException: Output column rawFeatures already exists.\n\tat org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:172)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:86)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-b200424e33e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     outputCol = 'rawFeatures')\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mspark_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorAssembler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mspark_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rawFeatures'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Output column rawFeatures already exists.'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols = inputCols,\n",
    "    outputCol = 'rawFeatures')\n",
    "\n",
    "spark_df = vectorAssembler.transform(spark_df)\n",
    "\n",
    "spark_df.select('rawFeatures').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Escalamiento de los datos\n",
    "##   Se procede al escalamiento de los datos relevantes para el agrupamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "##\n",
    "## Se construye el modelo y se entrena\n",
    "##\n",
    "scalerModel = MinMaxScaler(inputCol=\"rawFeatures\", outputCol=\"features\").fit(spark_df)\n",
    "\n",
    "##\n",
    "## Se aplica al DataFrame original para escalar los datos\n",
    "##\n",
    "spark_df = scalerModel.transform(spark_df)\n",
    "\n",
    "##\n",
    "## Se imprimen los datos escalados\n",
    "##\n",
    "spark_df.select(['features']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Determinación de los grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.4111397632582284\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "##\n",
    "## Se realiza el agrupamiento. SetK() indica la\n",
    "## cantidad de grupos para los que deben obtenerse\n",
    "## sus centros.\n",
    "##\n",
    "model = KMeans().setK(5).setSeed(1).fit(spark_df)\n",
    "\n",
    "\n",
    "spark_df = model.transform(spark_df)\n",
    "\n",
    "silhouette = ClusteringEvaluator().evaluate(spark_df)\n",
    "\n",
    "\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[0.03319446 0.00894352 0.01386418 0.00704525 0.00718048 0.00752434\n",
      " 0.00329701 0.00325638 0.00601355 0.00510883 0.00979218 0.01168149\n",
      " 0.00109098 0.00634891 0.00791208 0.00176525 0.01047724 0.00405721\n",
      " 0.00361525 0.00989082 0.00927761 0.00515989 0.00441119 0.00343609\n",
      " 0.0017008  0.00643388 0.00628235 0.00016319 0.0098997  0.01184277\n",
      " 0.         0.0031591  0.00252869 0.00657909 0.00662576 0.\n",
      " 0.00188643]\n",
      "[0.04732319 0.01824388 0.03681733 0.00983469 0.01404121 0.01513317\n",
      " 0.00610534 0.32093534 0.01156427 0.00508475 0.01400659 0.03185813\n",
      " 0.00263901 0.01129944 0.02429379 0.00727944 0.02231638 0.00395908\n",
      " 0.00218285 0.01167461 0.01715093 0.00770579 0.00834617 0.00386064\n",
      " 0.00243965 0.01786532 0.01647834 0.00088547 0.03495763 0.06471495\n",
      " 0.02683616 0.02354049 0.01977401 0.00866718 0.0090799  0.01059322\n",
      " 0.00512006]\n",
      "[0.05157551 0.01875615 0.02536873 0.01379875 0.01931286 0.02498946\n",
      " 0.00851651 0.00940675 0.00704277 0.0094002  0.01499508 0.04242871\n",
      " 0.00224603 0.01052114 0.0319469  0.00502609 0.02823009 0.00519353\n",
      " 0.00362027 0.0150212  0.01591516 0.00712072 0.01041164 0.00486726\n",
      " 0.00182355 0.01958862 0.04018355 0.00048533 0.08043265 0.15253419\n",
      " 0.01924779 0.02713864 0.02297198 0.01016358 0.01093552 0.00567847\n",
      " 0.00431416]\n",
      "[0.03767757 0.01132303 0.01799075 0.00882724 0.00908686 0.00860205\n",
      " 0.00518651 0.00525431 0.00669924 0.00592686 0.01718159 0.02360936\n",
      " 0.00719564 0.01492224 0.01664565 0.01581143 0.01784363 0.00648668\n",
      " 0.00338186 0.01583189 0.01948598 0.00930611 0.00600424 0.00405633\n",
      " 0.00298063 0.03004328 0.01800476 0.00117619 0.02690206 0.02361573\n",
      " 0.01875788 0.00735603 0.00732976 0.02117964 0.01657359 0.17946091\n",
      " 0.02088588]\n",
      "[0.03914672 0.01744742 0.02414277 0.01028256 0.01484647 0.01313849\n",
      " 0.00653796 0.00662843 0.00901052 0.00724028 0.01889739 0.03089016\n",
      " 0.00402871 0.01155726 0.0214914  0.01281071 0.02217973 0.00666898\n",
      " 0.00500608 0.01794336 0.01884731 0.00778856 0.00869981 0.00452518\n",
      " 0.00330262 0.02917679 0.02183981 0.00053444 0.03384321 0.04029202\n",
      " 0.16935946 0.016826   0.01281071 0.01303668 0.01193663 0.00707457\n",
      " 0.00803059]\n"
     ]
    }
   ],
   "source": [
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|         0|\n",
      "|         2|\n",
      "|         0|\n",
      "|         0|\n",
      "|         3|\n",
      "|         3|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         4|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         2|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.select('prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Análisis del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>21732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prediction  count\n",
       "0           1    708\n",
       "1           3   1592\n",
       "2           4   2607\n",
       "3           2   3361\n",
       "4           0  21732"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## Número de patrones asignados a cada cluster\n",
    "##\n",
    "spark_df.groupBy('prediction').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+-------+\n",
      "|prediction|gender|   age|friends|\n",
      "+----------+------+------+-------+\n",
      "|         0|     M|18.982|      7|\n",
      "|         2|     F|18.801|      0|\n",
      "|         0|     M|18.335|     69|\n",
      "|         0|     F|18.875|      0|\n",
      "|         3|    NA|18.995|     10|\n",
      "|         3|     F|  null|    142|\n",
      "|         0|     F| 18.93|     72|\n",
      "|         0|     M|18.322|     17|\n",
      "|         0|     F|19.055|     52|\n",
      "|         0|     F|18.708|     39|\n",
      "|         0|     F|18.543|      8|\n",
      "|         0|     F|19.463|     21|\n",
      "|         0|     F|18.097|     87|\n",
      "|         0|    NA|  null|      0|\n",
      "|         4|     F|18.398|      0|\n",
      "|         0|    NA|  null|      0|\n",
      "|         0|    NA|  null|    135|\n",
      "|         0|     F|18.987|     26|\n",
      "|         0|     F|17.158|     27|\n",
      "|         2|     F|18.497|    123|\n",
      "+----------+------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## clusters a los que pertenecen los primeros patrones\n",
    "##\n",
    "spark_df.select([\"prediction\", \"gender\", \"age\", \"friends\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gradyear: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- friends: integer (nullable = true)\n",
      " |-- basketball: integer (nullable = true)\n",
      " |-- football: integer (nullable = true)\n",
      " |-- soccer: integer (nullable = true)\n",
      " |-- softball: integer (nullable = true)\n",
      " |-- volleyball: integer (nullable = true)\n",
      " |-- swimming: integer (nullable = true)\n",
      " |-- cheerleading: integer (nullable = true)\n",
      " |-- baseball: integer (nullable = true)\n",
      " |-- tennis: integer (nullable = true)\n",
      " |-- sports: integer (nullable = true)\n",
      " |-- cute: integer (nullable = true)\n",
      " |-- sex: integer (nullable = true)\n",
      " |-- sexy: integer (nullable = true)\n",
      " |-- hot: integer (nullable = true)\n",
      " |-- kissed: integer (nullable = true)\n",
      " |-- dance: integer (nullable = true)\n",
      " |-- band: integer (nullable = true)\n",
      " |-- marching: integer (nullable = true)\n",
      " |-- music: integer (nullable = true)\n",
      " |-- rock: integer (nullable = true)\n",
      " |-- god: integer (nullable = true)\n",
      " |-- church: integer (nullable = true)\n",
      " |-- jesus: integer (nullable = true)\n",
      " |-- bible: integer (nullable = true)\n",
      " |-- hair: integer (nullable = true)\n",
      " |-- dress: integer (nullable = true)\n",
      " |-- blonde: integer (nullable = true)\n",
      " |-- mall: integer (nullable = true)\n",
      " |-- shopping: integer (nullable = true)\n",
      " |-- clothes: integer (nullable = true)\n",
      " |-- hollister: integer (nullable = true)\n",
      " |-- abercrombie: integer (nullable = true)\n",
      " |-- die: integer (nullable = true)\n",
      " |-- death: integer (nullable = true)\n",
      " |-- drunk: integer (nullable = true)\n",
      " |-- drugs: integer (nullable = true)\n",
      " |-- age1319: double (nullable = true)\n",
      " |-- rawFeatures: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- prediction: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.join(age_df, spark_df.gradyear == age_df.gradyear)\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|prediction|      avg(age1319)|\n",
      "+----------+------------------+\n",
      "|         1|16.982795681063113|\n",
      "|         3|17.393787091988127|\n",
      "|         4|17.115304945054945|\n",
      "|         2|17.101725897255488|\n",
      "|         0|17.292400662819276|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Características demográficas de los clusters.\n",
    "## Edad por cluster.\n",
    "##\n",
    "spark_df.groupby(\"prediction\").mean().select(['prediction', 'avg(age1319)']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Se agrega una columna indicadora para el genero\n",
    "##\n",
    "spark_df = spark_df.withColumn(\n",
    "    'isFemale',\n",
    "    when(spark_df['gender'] == 'F', 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|prediction|     avg(isFemale)|\n",
      "+----------+------------------+\n",
      "|         1|0.9067796610169492|\n",
      "|         3|0.7594221105527639|\n",
      "|         4|0.8304564633678557|\n",
      "|         2| 0.911335911930973|\n",
      "|         0|0.6890760169335542|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## Cantidad de mujeres por cluster.\n",
    "##\n",
    "spark_df.groupby(\"prediction\").mean().select(['prediction', 'avg(isFemale)']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
